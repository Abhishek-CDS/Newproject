{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecf7530",
   "metadata": {},
   "source": [
    "                                             __Bussiness Case__\n",
    "                                                                                                                           \n",
    "Bank GoodCredit wants to predict cred score for current credit card customers. The cred score will denote a customer’s credit worthiness and help the bank in reducing credit default risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd0d91",
   "metadata": {},
   "source": [
    "                                          ____DataSets Attributes____\n",
    "                                               \n",
    "Target variable → Bad_label\n",
    "* 0 – Customer has Good credit history\n",
    "* 1 – Customer has Bad credit history (falls into 30 DPD + bucket)\n",
    "\n",
    "This data has three tables and each tables has there own columns :-\n",
    "* __Cust_Account__   \n",
    "* __Cust_Demographics__ \n",
    "* __Cust_Enquiry__  \n",
    "\n",
    "                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5758a",
   "metadata": {},
   "source": [
    "                                                __Preface__\n",
    "                                              \n",
    "In bank Goodcredit, banks consider and evaluate every loan application based on merits. They check the creditworthiness of every individual or entity to determine the level of risk that they subject themself by lending to an entity or individual.Clients with a high level of risk are less desirable since they present with a high likelihood of defaulting on their loan obligations. Low-risk clients are more likely to get their loan applications approved since the lender considers them creditworthy.                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2aeb1",
   "metadata": {},
   "source": [
    "                                               __Domain Analysis__\n",
    "                                         \n",
    "Bank credit analysis involves verifying and determining the creditworthiness of a potential client by looking at their financial state, credit reports, and business cash flows.The goal of credit analysis is to determine the level of default risk that a client presents to the company and the losses that the bank will suffer if the client defaults.The risk level that a client presents determines whether the bank will approve or reject the loan application, and if approved, the amount to be awarded.some of the major table this data has and every tables has there own important columns which are Customers Account, Customers Demographics and Customers Enquiry.                                         \n",
    "\n",
    "* Cust_Account :- This table contains customer’s historical accounts data and payments history.\n",
    "* Cust_Demographics :- This table contains customer’s historical enquiry data such as enquiry amount and enquiry purpose.\n",
    "* Cust_Enquiry :- Current customer applications with demographic data\n",
    "* __Note__ that demographics features are renamed as features and obscured in accordance with privacy policies.\n",
    "\n",
    "* I will explain every necesseary columns when i'll do __Data Cleaning__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12dfd9",
   "metadata": {},
   "source": [
    "                                              __Data Set Fields__\n",
    "                                              \n",
    "For getting a Dataset we have to install pymysql and mysql connector. and the help of mysql i'm going to load dataset and convert into csv file then i'm going to work on this project.                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a585ea",
   "metadata": {},
   "source": [
    "                                            __python implimentation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f725f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: mysql-connector in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.2.9)\n"
     ]
    }
   ],
   "source": [
    "# installing pymysql and mysql-connector package for making connections to database server\n",
    "\n",
    "!pip install pymysql  \n",
    "!pip install mysql-connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06511496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os               # Handling the Current Working Directory\n",
    "import mysql.connector  #  mysql-connector for making connections to database server\n",
    "import pandas as pd     # import pandas for analyzing, cleaning, exploring, and manipulating data\n",
    "import numpy as np      # import numpy for working with mathematices part/numerical data\n",
    "## visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# for ignoring warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making connections to database server\n",
    "connection=mysql.connector.connect(host = \"18.136.157.135\",\n",
    "                                  user='dm_team1',\n",
    "                                  password='DM!$Team&279@20!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f25b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the number of databases available on the server\n",
    "cursor=connection.cursor()\n",
    "cursor.execute('show databases')\n",
    "for i in cursor:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## connecting the database\n",
    "connection=mysql.connector.connect(host='18.136.157.135',\n",
    "                                  user='dm_team1',\n",
    "                                  password='DM!$Team&279@20!',\n",
    "                                  database='project_banking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all tables of bank goodcredit\n",
    "db_tables=pd.read_sql_query('show tables',connection)\n",
    "print(db_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71abf04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put Cust_Account tables in variable\n",
    "Cust_Account = pd.read_sql_query(\"Select * from Cust_Account\",connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d478990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put Cust_Demographics tables in variable\n",
    "Cust_Demographics = pd.read_sql_query(\"Select * from Cust_Demographics\",connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09204c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put Cust_Enquiry tables in variable\n",
    "# Don't need Customers enquriy for traing model so we are skip the tables\n",
    "# we skip this tables because customers enquiry tables nevers contribute for checking credit score\n",
    "Cust_Enquiry = pd.read_sql_query(\"Select * from Cust_Enquiry\",connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b071812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a code for showing all columns\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.reset_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This table contains customer’s historical accounts data and payments history.\n",
    "Cust_Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad9142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This table contains customer’s historical enquiry data such as enquiry amount and enquiry purpose.\n",
    "Cust_Demographicsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current customer applications with demographic data\n",
    "Cust_Enquiry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i'm going to merge the all tables in one variables and making one dataset of two tables by using inner join\n",
    "data = pd.merge(Cust_Account,Cust_Demographics,on =\"customer_no\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b2839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's we got dataset after mergeing.....\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ac7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save mergeing dataset\n",
    "data.to_csv(\"Bank_GoodCredits\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is in csv file so we have to call pd.read_csv from pandas\n",
    "data = pd.read_csv(\"Bank_GoodCredits\")# loading a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4dd58",
   "metadata": {},
   "source": [
    "# Basic Checks :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a38926",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()  # information of each and every columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()    #statistical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=\"O\")  # statistical information of categorical data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb38aa05",
   "metadata": {},
   "source": [
    "Insight from Basic checks :-\n",
    "\n",
    "* we can see that this dataset has approx 50 lakhs rows.\n",
    "* there are lots of columns which are unnecessary columns we have to drop.\n",
    "* Lots of the columns having object type we have to change into int or float.\n",
    "* In this dataset lots of the missing are availables we cannot do EDA first.So,here we are going to do data cleaning         first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842be53",
   "metadata": {},
   "source": [
    "# Data Preprocessing :-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c306d99c",
   "metadata": {},
   "source": [
    "Handling missing value first and going to drop columns which has around 40% missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bcdb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()  # checking null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac18db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i'm running a loop which columns having >=40% missing value i'm append in new variable and drop that columns.\n",
    "df = []\n",
    "for i in data.columns:\n",
    "    if (data[i].isnull().sum())/len(data)*100>=40:\n",
    "        df.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping columns which has >=40% missing value\n",
    "data.drop(['closed_dt', 'amt_past_due', 'paymenthistory2', 'creditlimit', 'cashlimit', 'rateofinterest', 'paymentfrequency',\n",
    "           'actualpaymentamount', 'feature_8', 'feature_9', 'feature_10', 'feature_13', 'feature_17', 'feature_18',\n",
    "           'feature_45', 'feature_48', 'feature_49', 'feature_51', 'feature_53', 'feature_57', 'feature_61', 'feature_73',\n",
    "           'feature_74','Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking duplicate rows\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicated rows\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab405ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to drop unnecessary columns\n",
    "\n",
    "data.drop([\"reporting_dt\",\"paymenthistory1\",\"dt_opened_y\",\"entry_time\",\"feature_2\",\"feature_20\",\"feature_21\",\n",
    "          \"feature_22\",\"feature_24\",\"feature_46\",\"feature_47\",\"feature_54\",\"feature_75\",\"feature_77\",\"last_paymt_dt\",\"paymt_str_dt\",\"paymt_end_dt\",\"feature_5\",\"feature_6\",\"feature_62\",\"feature_70\",\n",
    "           \"feature_79\",\"feature_39\",\"dt_opened_x\",\"upload_dt\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b2b01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run a loop for getting all columns unique values\n",
    "\n",
    "for i in data.columns:\n",
    "    print(f\"======= { i } ========\\n\")\n",
    "    print(data[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, i'm going to drop that columns which has no variety in values\n",
    "\n",
    "data.drop([\"owner_indic\",\"feature_11\",\"feature_19\",\"feature_23\",\"feature_25\",\"feature_31\",\"feature_33\",\"feature_42\",\n",
    "          \"feature_55\",\"feature_58\",\"feature_59\",\"feature_60\",\"feature_67\",\"feature_76\",\"feature_78\",\"feature_52\",],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79d9408",
   "metadata": {},
   "source": [
    "### filling the null value and change data type :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()  # checking null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling all null value by the help of Measure of Central Tendency\n",
    "\n",
    "\n",
    "data.loc[data[\"opened_dt\"].isnull(),\"opened_dt\"] = \"13-Apr-12\"\n",
    "data.loc[data[\"high_credit_amt\"].isnull(),\"high_credit_amt\"] = np.mean(data.high_credit_amt)\n",
    "data.loc[data[\"feature_1\"].isnull(),\"feature_1\"] = \"Platinum Maxima\"\n",
    "data.loc[data[\"feature_3\"].isnull(),\"feature_3\"] = 682.0\n",
    "data.loc[data[\"feature_4\"].isnull(),\"feature_4\"] = 3.000000\n",
    "data.loc[data[\"feature_7\"].isnull(),\"feature_7\"] = np.mean(data.feature_7)\n",
    "data.loc[data[\"feature_12\"].isnull(),\"feature_12\"] = \"PM1\"\n",
    "data.loc[data[\"feature_14\"].isnull(),\"feature_14\"] = 12.000000\n",
    "data.loc[data[\"feature_15\"].isnull(),\"feature_15\"] = \"SA03\"\n",
    "data.loc[data[\"feature_16\"].isnull(),\"feature_16\"] = \"AS03\"\n",
    "data.loc[data[\"feature_26\"].isnull(),\"feature_26\"] = 0.000000\n",
    "data.loc[data[\"feature_27\"].isnull(),\"feature_27\"] = \"Graduate\"\n",
    "data.loc[data[\"feature_28\"].isnull(),\"feature_28\"] = \"New Delhi\"\n",
    "data.loc[data[\"feature_29\"].isnull(),\"feature_29\"] = np.mean(data.feature_29)\n",
    "data.loc[data[\"feature_30\"].isnull(),\"feature_30\"] = 2010.0\n",
    "data.loc[data[\"feature_32\"].isnull(),\"feature_32\"] = \"Self\"\n",
    "data.loc[data[\"feature_34\"].isnull(),\"feature_34\"] = 2.0\n",
    "data.loc[data[\"feature_35\"].isnull(),\"feature_35\"] = 42759.59392727169\n",
    "data.loc[data[\"feature_36\"].isnull(),\"feature_36\"] = \"Private Ltd. Co.\"\n",
    "data.loc[data[\"feature_37\"].isnull(),\"feature_37\"] = \"Banking/Financial Services\"\n",
    "data.loc[data[\"feature_38\"].isnull(),\"feature_38\"] = \"MANAGER\"\n",
    "data.loc[data[\"feature_40\"].isnull(),\"feature_40\"] = 0.0\n",
    "data.loc[data[\"feature_41\"].isnull(),\"feature_41\"] = 11.0\n",
    "data.loc[data[\"feature_43\"].isnull(),\"feature_43\"] = \"New Delhi\"\n",
    "data.loc[data[\"feature_44\"].isnull(),\"feature_44\"] = np.mean(data.feature_44)\n",
    "data.loc[data[\"feature_50\"].isnull(),\"feature_50\"] = \"Y\"\n",
    "data.loc[data[\"feature_63\"].isnull(),\"feature_63\"] = \"2010-0\"\n",
    "data.loc[data[\"feature_64\"].isnull(),\"feature_64\"] = 10.0\n",
    "data.loc[data[\"feature_65\"].isnull(),\"feature_65\"] = 157.0\n",
    "data.loc[data[\"feature_66\"].isnull(),\"feature_66\"] = np.mean(data.feature_66)\n",
    "data.loc[data[\"feature_68\"].isnull(),\"feature_68\"] = 1.0\n",
    "data.loc[data[\"feature_69\"].isnull(),\"feature_69\"] = np.mean(data.feature_69)\n",
    "data.loc[data[\"feature_71\"].isnull(),\"feature_71\"] = 10.0\n",
    "data.loc[data[\"feature_72\"].isnull(),\"feature_72\"] = \"R\"\n",
    "data.loc[data[\"feature_56\"].isnull(),\"feature_56\"] = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722c92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()  # checking null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b57774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Date columns in date formate using pd.to_datetime technique\n",
    "def convert_into_datetime(columns):\n",
    "    data[columns]=pd.to_datetime(data[columns])\n",
    "    \n",
    "# Converting the columns into Datetime using for loop\n",
    "for i in ['opened_dt']:  \n",
    "    convert_into_datetime(i) \n",
    "    \n",
    "# Create the new columns with month \n",
    "data['opened_dt_year']=data['opened_dt'].dt.year\n",
    "\n",
    "data.drop('opened_dt',axis=1,inplace=True)  # drop opened_dt columns because now it's not necessary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54977301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i'm going to split the feature_63 columns for getting only year\n",
    "\n",
    "data.feature_63 = data['feature_63'].str.split('-',expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bededed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86483192",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"feature_3\"]<500,\"feature_3\"]=np.median(data.feature_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec658e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change columns type according to their value some columns having object type but it should in int type \n",
    "\n",
    "data.high_credit_amt = data.high_credit_amt.astype(int)\n",
    "data.feature_3 = data.feature_3.astype(int)\n",
    "data.feature_4 = data.feature_4.astype(int)\n",
    "data.feature_14 = data.feature_14.astype(int)\n",
    "data.feature_26 = data.feature_26.astype(int)\n",
    "data.feature_30 = data.feature_30.astype(int)\n",
    "data.feature_34 = data.feature_34.astype(int)\n",
    "data.feature_40 = data.feature_40.astype(int)\n",
    "data.feature_44 = data.feature_41.astype(int)\n",
    "data.feature_56 = data.feature_56.astype(int)\n",
    "data.feature_63 = data.feature_63.astype(int)\n",
    "data.feature_64 = data.feature_64.astype(int)\n",
    "data.feature_65 = data.feature_65.astype(int)\n",
    "data.feature_68 = data.feature_68.astype(int)\n",
    "data.feature_71 = data.feature_71.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26431a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72797ffb",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "* we can't do EDA with this dataset because most of the columns name are hide because of security propose but we are going to do EDA with possible columns which can give us perfect insights.\n",
    "* Now,lets start EDA after that we are going to change categorical variable into numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d598fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data.feature_1,hue = data.Bad_label)  #assign countplot between feature_1 and Bad Labels\n",
    "plt.title(\"Credit Cards vs Bad Labels\") # assign title name of distributions\n",
    "plt.xticks(rotation=90)#increasing font of x-axis and rorating 90 degree \n",
    "plt.show() # for showing graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36bee2",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "\n",
    "* Most of the person having Platinum Maxima and Platinum Deligh Cards.\n",
    "* person who has Golf Card they having 50-50 chance of good or bad Credit Score (bad or good labels).\n",
    "* person who have RBL bank Fun+,Insignia and Platinum Cricke Cards they have perfect credit score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313cd98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data.feature_3)# asign histplot for feature_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1670ff",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "* Most of the customers have good credit score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e42310",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data.feature_27,hue = data.Bad_label)#assign countplot between feature_27 and Bad Labels\n",
    "plt.title(\"customers education vs Bad Labels\")# assign title name of distributions\n",
    "plt.xticks(rotation=90)#increasing font of x-axis and rorating 90 degree \n",
    "plt.show()# for showing graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c922de",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "* Few chance of that customers who has completed Post Graduate, Graduate and MBA/MMS their Credit score are Bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data.feature_28,hue = data.Bad_label)#assign countplot between feature_28 and Bad Labels\n",
    "plt.title(\"Customer State vs Bad Labels\") # assign title name of distributions\n",
    "plt.xticks(fontsize = 7,rotation = 90)#increasing font of x-axis and rorating 90 degree \n",
    "plt.yticks(fontsize = 11)#increasing font of y-axis\n",
    "plt.show()# for showing graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3839f1",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "* Most of the Customers belong from New Delhi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c26e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data.feature_32,hue = data.Bad_label)#assign countplot between feature_32 and Bad Labels\n",
    "plt.title(\"Customers stay vs Bad Labels\") # assign title name of distributions\n",
    "plt.xticks(rotation=90)#increasing font of x-axis and rorating 90 degree \n",
    "plt.show()# for showing graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd484e2",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "* Most of the customers who has crdit cards they stay at their own house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86526a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data.feature_36,hue = data.Bad_label) #assign countplot between feature_36 and Bad Labels\n",
    "plt.title(\"customers Business vs Bad Labels\") # assign title name of distributions\n",
    "plt.xticks(rotation=90)#increasing font of x-axis and rorating 90 degree \n",
    "plt.show()# for showing graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a794c",
   "metadata": {},
   "source": [
    "### Insights \n",
    "\n",
    "\n",
    "* Most of the Customer having Credit Cards who belong from Private Ltd.Co.\n",
    "* Customers who started their bussiness in Partership they maintain their credit score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data.feature_37,hue = data.Bad_label) #assign countplot between feature_37 and Bad Labels\n",
    "plt.title(\"customers Ocupations vs Bad Labels\") # assign title name of distributions\n",
    "plt.xticks(rotation=90)#increasing font of x-axis and rorating 90 degree \n",
    "plt.show()# for showing graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c790da7",
   "metadata": {},
   "source": [
    "### Insights \n",
    "\n",
    "* Most chance of that Customers who doing job in Banking/Financial services they taking credit cards. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4a8ad",
   "metadata": {},
   "source": [
    "# Final Conclusions of Insights :-\n",
    "\n",
    "\n",
    "* Most of the person having Platinum Maxima and Platinum Deligh Cards.\n",
    "* Person who has Golf Card they having 50-50 chance of good or bad Credit Score (bad or good labels).\n",
    "* Person who have RBL bank Fun+,Insignia and Platinum Cricke Cards they have perfect credit score.\n",
    "* Most of the customers have good credit score.\n",
    "* Few chance of that customers who has completed Post Graduate, Graduate and MBA/MMS their Credit score are Bad.\n",
    "* Most of the Customers belong from New Delhi.\n",
    "* Most of the Customer having Credit Cards who belong from Private Ltd.Co.\n",
    "* Customers who started their bussiness in Partership they maintain their credit score.\n",
    "* Most chance of that Customers who doing job in Banking/Financial services they taking credit cards.\n",
    "* Most of the customers who has crdit cards they stay at their on home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef4fde6",
   "metadata": {},
   "source": [
    "#### Checking Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3bbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting that columns which type are integers\n",
    "\n",
    "df = data.select_dtypes(include='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c314d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data.high_credit_amt)  # asign boxplot of high credit amount for checking outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34889f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed outliers with Measure of central tendency\n",
    "\n",
    "data.loc[data[\"high_credit_amt\"]>100000000,\"high_credit_amt\"] = np.mean(data.high_credit_amt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b2c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data.cur_balance_amt)# asign boxplot of current balance amount for checking outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed outliers with Measure of central tendency\n",
    "data.loc[data[\"cur_balance_amt\"]>130000000,\"cur_balance_amt\"] = np.mean(data.cur_balance_amt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95052d",
   "metadata": {},
   "source": [
    "# Conversion of categorical variables\n",
    "\n",
    "* Here i'm going to apply LabelEncoder and manually Encoding because every data are Nominal Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59446cb4",
   "metadata": {},
   "source": [
    "#### Feature_1\n",
    "\n",
    "* which showing cards types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c797787",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b622128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual encoding of Total stops\n",
    "data.feature_1 = data.feature_1.replace({\"Platinum Cricke\":0,\"Golf Card\":1,\"Insignia\":2,\"RBL Bank Fun+\":3,\"Titanium Deligh\":4,\n",
    "                                        \"Platinum Deligh\":5,\"Platinum Maxima\":6})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba0d51c",
   "metadata": {},
   "source": [
    "#### Feature_27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21208486",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_27.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3148616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual encoding of feature_27\n",
    "\n",
    "data.feature_27 = data.feature_27.replace({\"Architect\":0,\"CA\":1,\"Engineer\":2,\"Professional\":3,\"Doctor\":4,\"Others\":5,\n",
    "                                           \"Diploma\":6,\"MBA/MMS\":7,\"Post-Graduate\":8,\"Graduate\":9})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7b7a0",
   "metadata": {},
   "source": [
    "#### Feature_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a81db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_32.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #manual encoding of feature_32\n",
    "data.feature_32 = data.feature_32.replace({\"PG/Ho\":0,\"Compa\":1,\"Rente\":2,\"Paren\":3,\"Self\":4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0cd34",
   "metadata": {},
   "source": [
    "#### Feature_36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_36.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_36\n",
    "data.feature_36 = data.feature_36.replace({\"Others\":0,\"Partnership\":1,\"PSU\":2,\"Partnership Co.\":3,\"Government Organisation\":4,\n",
    "                                          \"Proprietorship\":5,\"MNC\":6,\"Public Ltd Co.\":7,\"Private Ltd. Co.\":8})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f428f1",
   "metadata": {},
   "source": [
    "#### Feature_37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f93309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_37.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8255347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #manual encoding of feature 37\n",
    "data.feature_37 = data.feature_37.replace({\"Agriculture\":0,\"Tourism\":1,\"Real Estate\":2,\"Transportation/Logistics\":3,\"Manufacturing\":4,\n",
    "                                          \"Health Care\":5,\"KPO/LPO/ITES\":6,\"Industrial\":7,\"Information Technology\":8,\"Others\":9,\n",
    "                                          \"Banking/Financial Services\":10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b394e",
   "metadata": {},
   "source": [
    "#### Feature_72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_72.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91022d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #manual encoding of feature 72\n",
    "data.feature_72 = data.feature_72.replace({\"O\":0,\"R\":1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866cf98b",
   "metadata": {},
   "source": [
    "#### Feature_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb4c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_12.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99141a",
   "metadata": {},
   "source": [
    "#### Feature_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_50.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba44701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual encoding of feature 50\n",
    "\n",
    "data.feature_50 = data.feature_50.replace({\"N\":0,\"Y\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a473d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder  # assign label encoder for change categorical value into numerical\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d487eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_12 = le.fit_transform(data.feature_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84281f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_15 = le.fit_transform(data.feature_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b636c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_16 = le.fit_transform(data.feature_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10470447",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_28 = le.fit_transform(data.feature_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_38 = le.fit_transform(data.feature_38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f87ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_50.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i'm going to drop feature_43 variable because feature_28 and feature_43 are giving us same information.\n",
    "\n",
    "data.drop(\"feature_43\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7379ee",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7fba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()  # checking corelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28,28))#increase plot size\n",
    "sns.heatmap(data.corr(),annot=True,cmap=\"RdYlGn\")#its show corelation between each and every columns by plot heatmap\n",
    "#here we can see that every columns are connected to each other,So we can't remove any columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8389b",
   "metadata": {},
   "source": [
    "* Here, we can see that feature_30 highly corelated with feature_63\n",
    "* feature_34 highly corelated with feature_68\n",
    "* feature_35 highly corelated with feature_69\n",
    "* feature_41 highly corelated with feature_44\n",
    "* So, according to statistical way i'm going to drop one feature of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"feature_63\",\"feature_68\",\"feature_69\",\"feature_41\"],axis=1,inplace=True) # droping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5776ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now selecting independent variable and dependent variables\n",
    "\n",
    "x = data.drop(\"Bad_label\",axis=1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110986da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.Bad_label\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target variable are not balanced so we have to balanced this for avoiding overfitting and underfitting\n",
    "data.Bad_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb859f",
   "metadata": {},
   "source": [
    "# Model Creation :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # import train test split for spliting the data\n",
    "# creating x_train,x_test,y_train,y_test\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=43) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e0da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97439f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b805e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7758f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c2c54",
   "metadata": {},
   "source": [
    "# Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdfbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE  # assign SMOTE for handling unbalanced data\n",
    "sm = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19649d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sm,y_sm = sm.fit_resample(x_train,y_train)  # balancing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter  # checking data balanced or not\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52222703",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c733e037",
   "metadata": {},
   "source": [
    "## LogisticRegression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f93484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression  # assign Logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_sm,y_sm) # fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ca1eb",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab243dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier   # assign decision tree classifier algorithmn\n",
    "dt = DecisionTreeClassifier()                    \n",
    "dt.fit(x_sm,y_sm)   # fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff1536",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier#importing randomforest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)#object creation ,taking 100 decision tree in random forest \n",
    "rf_clf.fit(x_sm,y_sm)#training the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377836b8",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xg    #importing xgboost with alias name xg\n",
    "from xgboost import XGBClassifier   #calling XGBRegressor\n",
    "xgb_r = xg.XGBClassifier(    #assign in variable\n",
    "                  n_estimators = 500, seed = 123)\n",
    "xgb_r.fit(x_sm, y_sm)# Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec402b",
   "metadata": {},
   "source": [
    "## MLPClassifier -ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model creation\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier( hidden_layer_sizes=(100,2),\n",
    "                       learning_rate_init=0.1,\n",
    "                       max_iter=150,random_state=33) ## model object creation max_iter=Stopping parameter\n",
    "model.fit(x_sm,y_sm) ## training the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddcf63",
   "metadata": {},
   "source": [
    "## Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign evalution metrics for checking performance of model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,r2_score,f1_score,precision_score,recall_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8084c030",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51104fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(x_test) # predict the model\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values # checking test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_pred,y_test) # checking accuracy score\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_pred,y_test,average=\"weighted\") # checking f1 score\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_pred,y_test) # checking confusion metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred,y_test)) # checking classification report of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6da45d",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4=dt.predict(x_test) # predict the model\n",
    "y_pred4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values # checking y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc5 = accuracy_score(y_pred4,y_test) # checking accuracy score\n",
    "f1_2 = f1_score(y_pred4,y_test)\n",
    "print(\"accuracy_score :\", accuracy_score(y_pred4,y_test))\n",
    "print(\"precision_score :\", precision_score(y_pred4,y_test))\n",
    "print(\"recall_score :\", recall_score(y_pred4,y_test))\n",
    "print(\"F1_score :\", f1_score(y_pred4,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ad0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred4,y_test)) # checking classification report for model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47375b19",
   "metadata": {},
   "source": [
    "## RandomForest Classifier Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712416f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=rf_clf.predict(x_test)#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc4 = accuracy_score(y_test,y_predict)# checking accuracy\n",
    "f1_3 = f1_score(y_test,y_predict)\n",
    "print(\"accuracy_score :\", accuracy_score(y_test,y_predict))\n",
    "print(\"precision_score :\", precision_score(y_test,y_predict))\n",
    "print(\"recall_score :\", recall_score(y_test,y_predict))\n",
    "print(\"F1_score :\", f1_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2023ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_predict))# checking classification report for model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfecaa6",
   "metadata": {},
   "source": [
    "## XGBoost Classifier Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb_r.predict(x_test)# Predict the model\n",
    "\n",
    "acc_score4 = accuracy_score(y_test,xgb_pred)  #checking r2 score\n",
    "f1_4 = f1_score(y_test,xgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcf398",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy_score :\", accuracy_score(y_test,xgb_pred))\n",
    "print(\"precision_score :\", precision_score(y_test,xgb_pred))\n",
    "print(\"recall_score :\", recall_score(y_test,xgb_pred))\n",
    "print(\"F1_score :\", f1_score(y_test,xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57563436",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,xgb_pred))# checking classification report for model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea71e912",
   "metadata": {},
   "source": [
    "# ANN - MLPClassifier Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e019c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_proba = model.predict(x_test) ## predicting the probability of class\n",
    "y_train_predict = model.predict(x_train) # predicting the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc4 = accuracy_score(y_test,y_predict_proba)# checking accuracy score\n",
    "f1_5 = f1_score(y_test,y_predict_proba)\n",
    "acc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc329b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_predict_proba))# checking classification report for model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d441515",
   "metadata": {},
   "source": [
    "# Model Comparsion Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model comparison report using DataFrame\n",
    "score = pd.DataFrame({\"Model\":[\"LogisticRegression\",\"DecisionTreeClassifier\",\"RandomForestClassifier\",\"XGBoost Classifier\",\"ANN_MLPClassifier\"],\n",
    "                     \"Accuracy\":[acc*100,acc5*100,acc4*100,acc_score4*100,acc4*100],\n",
    "                     \"f1_score\":[f1,f1_2,f1_3,f1_4,f1_5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "score# showing model comparison report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1754455",
   "metadata": {},
   "source": [
    "# Conclusion of Model Comparison Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e212b",
   "metadata": {},
   "source": [
    "I have used 5 Algorithmns which name are LogisticRegression,DecisionTreeClassifier,RandomForestClassifier,XG Boost and ANN_MLPClassifier for training the model. I got __99.73__ percentage in __XGBoost Classifier__ which are maximum than all Algorithmn and its working Mindblowing and model predict perfect result. So,I am perfering XGBoost Classifier for checking the __GoodCredit Score of customers__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135bf4a",
   "metadata": {},
   "source": [
    "# Data Analysis Report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7184dc",
   "metadata": {},
   "source": [
    "The project is a machine learning model to classify Bank GoodCredit Score. The dataset contained 3 tables of Bank      GoodCredit Score which are Customers Enquiry,customers Demographics and Customers Account. The project was divided into several steps, including data exploration, data preprocessing, building and training machine learning models, and evaluating model performance.\n",
    "\n",
    "During data exploration, we analyzed the dataset to get a better understanding of the data. We observed that the dataset was not balanced,target labels was unbalanced. We also noticed that this dataset contains lots of null value and we have to fill and drop columns which has more than 40% null value,Even most columns had wrong datatype So,we need to fixed that columns and change categorical columns into numerical for training.\n",
    "\n",
    "For data preprocessing, We have fill null and droped that columns which has more than 40% missing value. We also droped that columns which has no variaty in values for reduced overfitting and underfitting of model.we also fixed wrong datatypes of columns and we used LabelsEncoders and manually encoding for change categorical variables into numerical variables.Even we droped unnecessary variables.\n",
    "\n",
    "We built machine learning models by applying LogisticRegression,DecisionTreeClassifier,RandomForestClassifier,XG Boost and ANN_MLPClassifier for training the model . We trained these models on the original datasets and evaluated their performance using accuracy, recall score, precision score and F1 score. We also plotted the training and validation curves to analyze the models' behavior during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875f4a8",
   "metadata": {},
   "source": [
    "# Report on Challenges faced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68b048",
   "metadata": {},
   "source": [
    "* __Huge amount of data__ : this dataset contains 3 tables after merging important tables rows goes to 1.9 lakhs and         columns goes to 105 which are huge because of this we faced more problem while doing work it was take more time for run     any cells.\n",
    "\n",
    "* __Choosing columns for training model__ : this dataset has 105 columns and most of the columns having no variety in their   values it was difficult to select important columns.\n",
    "\n",
    "* __hiding columns names__ : this dataset has customers demographices tables which columns name are hiding for security       purpose.it was difficult to understand what columns are saying because of this it was difficult to select important         columns.\n",
    "\n",
    "* __No EDA__ : we can't do EDA in this dataset because 80% columns names are hide and it's difficult to get insights from     data.because of this we got problem while doing EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1064c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
